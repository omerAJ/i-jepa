{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "script_dir = os.path.dirname(r\"D:\\omer\\i-jepa\\ijepa\\src\\linearProbing.py\")  # Get the directory where the script is located\n",
    "parent_dir = os.path.dirname(script_dir)  # Get the parent directory\n",
    "parent_dir2 = os.path.dirname(parent_dir)  # Get the parent directory\n",
    "sys.path.insert(0, parent_dir)\n",
    "sys.path.insert(0, parent_dir2)\n",
    "from src.helper import (\n",
    "    load_checkpoint,\n",
    "    init_model,\n",
    "    init_opt)\n",
    "\n",
    "device=torch.device('cuda:0')\n",
    "patch_size=14\n",
    "crop_size=224\n",
    "pred_depth=12\n",
    "pred_emb_dim=384\n",
    "model_name='vit_tiny'\n",
    "load_path=r\"D:\\omer\\i-jepa\\ijepa\\logs\\jepa-latest.pth.tar\"\n",
    "\n",
    "\n",
    "encoder, predictor = init_model(\n",
    "        device=device,\n",
    "        patch_size=patch_size,\n",
    "        crop_size=crop_size,\n",
    "        pred_depth=pred_depth,\n",
    "        pred_emb_dim=pred_emb_dim,\n",
    "        model_name=model_name)\n",
    "target_encoder = copy.deepcopy(encoder)\n",
    "\n",
    "encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "            device=device,\n",
    "            r_path=load_path,\n",
    "            encoder=encoder,\n",
    "            predictor=predictor,\n",
    "            target_encoder=target_encoder,\n",
    "            opt=None,\n",
    "            scaler=None)\n",
    "\n",
    "class EncoderWithLinearHead(nn.Module):\n",
    "    def __init__(self, encoder, output_dim):\n",
    "        super(EncoderWithLinearHead, self).__init__()\n",
    "        self.encoder = encoder \n",
    "        self.head = nn.Linear(49152, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x) \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.head(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "num_classes = 10  # Number of output classes, e.g., 10 for MNIST\n",
    "model = EncoderWithLinearHead(encoder, num_classes).to(device)\n",
    "\n",
    "# If you want to freeze the encoder weights\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "print(encoder)\n",
    "\n",
    "import torchvision\n",
    "import sys\n",
    "import os\n",
    "from transforms import make_transforms\n",
    "\n",
    "crop_scale=(1.0, 1.0)\n",
    "transform = make_transforms(\n",
    "        crop_size=crop_size,\n",
    "        crop_scale=crop_scale,\n",
    "        gaussian_blur=False,\n",
    "        horizontal_flip=False,\n",
    "        color_distortion=False,\n",
    "        color_jitter=False)\n",
    "batch_size=4096\n",
    "trainset = torchvision.datasets.MNIST(root=r'D:/omer/i-jepa/ijepa/src/datasets/mnist', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=r'D:/omer/i-jepa/ijepa/src/datasets/mnist', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: \", device)\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 2\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch}/{epochs}\")\n",
    "    model.train()\n",
    "    for data in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data[0].to(device))\n",
    "        loss = loss_fn(out, data[1].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    if epoch%2==0:\n",
    "        model.eval()\n",
    "        for data in tqdm(testloader, desc=f\"Epoch {epoch+1}/{epochs} - Testing\"):\n",
    "            out = model(data[0].to(device))\n",
    "            loss = loss_fn(out, data[1].to(device))\n",
    "            test_loss.append(loss.item())\n",
    "\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(train_loss, label='Training Loss')\n",
    "    # plt.plot(test_loss, label='Testing Loss')\n",
    "    # plt.xlabel('Iterations')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title(f'Epoch {epoch+1} Training Loss')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\omer\\i-jepa\\ijepa\\src\\linearProbing.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/omer/i-jepa/ijepa/src/linearProbing.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/omer/i-jepa/ijepa/src/linearProbing.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/omer/i-jepa/ijepa/src/linearProbing.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/omer/i-jepa/ijepa/src/linearProbing.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m all_preds \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/omer/i-jepa/ijepa/src/linearProbing.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m all_targets \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        preds = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "        all_preds.extend(preds.view_as(target).cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i-jepaVENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
